{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Baseline\n",
    "\n",
    "This notebook shows how to train the baseline model for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.train import TrainConfig, run_train_model\n",
    "from utils.augmentations import get_default_transform\n",
    "from utils import creating_dataset\n",
    "\n",
    "# this is the implementation of the custom baseline model\n",
    "from utils import hvatnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainer configuration\n",
    "\n",
    "The `TrainConfig` class is used to train the baseline model - have a look at the parameters it has!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(exp_name='test_2_run_fedya', p_augs=0.3, batch_size=64, eval_interval=150, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting val datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Getting train datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Number of moves: 70 | Dataset: valery_first_standart_elbow_left\n",
      "Reorder this dataset valery_first_standart_elbow_left True\n",
      "Number of moves: 135 | Dataset: alex_kovalev_standart_elbow_left\n",
      "Reorder this dataset alex_kovalev_standart_elbow_left True\n",
      "Number of moves: 72 | Dataset: anna_makarova_standart_elbow_left\n",
      "Reorder this dataset anna_makarova_standart_elbow_left True\n",
      "Number of moves: 62 | Dataset: artem_snailbox_standart_elbow_left\n",
      "Reorder this dataset artem_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: matthew_antonov_standart_elbow_left\n",
      "Reorder this dataset matthew_antonov_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_left\n",
      "Reorder this dataset misha_korobok_standart_elbow_left True\n",
      "Number of moves: 71 | Dataset: nikita_snailbox_standart_elbow_left\n",
      "Reorder this dataset nikita_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: petya_chizhov_standart_elbow_left\n",
      "Reorder this dataset petya_chizhov_standart_elbow_left True\n",
      "Number of moves: 12 | Dataset: polina_maksimova_standart_elbow_left\n",
      "Reorder this dataset polina_maksimova_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: sema_duplin_standart_elbow_left\n",
      "Reorder this dataset sema_duplin_standart_elbow_left True\n",
      "Number of moves: 136 | Dataset: alex_kovalev_standart_elbow_right\n",
      "Number of moves: 69 | Dataset: andrew_snailbox_standart_elbow_right\n",
      "Number of moves: 132 | Dataset: anna_makarova_standart_elbow_right\n",
      "Number of moves: 67 | Dataset: artem_snailbox_standart_elbow_right\n",
      "Number of moves: 68 | Dataset: matthew_antonov_standart_elbow_right\n",
      "Number of moves: 72 | Dataset: matvey_gorbenko_standart_elbow_right\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_right\n",
      "Number of moves: 55 | Dataset: nikita_snailbox_standart_elbow_right\n",
      "Number of moves: 142 | Dataset: petya_chizhov_standart_elbow_right\n",
      "Number of moves: 54 | Dataset: polina_maksimova_standart_elbow_right\n",
      "Number of moves: 139 | Dataset: sema_duplin_standart_elbow_right\n",
      "Number of trainining sessions: 22\n",
      "Number of validation sessions: 1\n",
      "Size of the input (8, 256) || Size of the output (20, 32)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"/disk/scratch_fast/nkudryas/dataset_v2_blocks\"\n",
    "\n",
    "def count_parameters(model): \n",
    "    n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n_total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total: {n_total/1e6:.2f}M, Trainable: {n_trainable/1e6:.2f}M\")\n",
    "    return n_total, n_trainable\n",
    "\n",
    "\n",
    "    \n",
    "## Data preparation\n",
    "transform = get_default_transform(train_config.p_augs)\n",
    "data_paths = dict(datasets=[DATA_PATH],\n",
    "                    hand_type = ['left', 'right'], # [left, 'right']\n",
    "                    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "                    test_dataset_list = ['fedya_tropin_standart_elbow_left'])\n",
    "data_config = creating_dataset.DataConfig(**data_paths)\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "As you can see below, the model has a number of hyperparameters specifying its architecture and parameters. These are the parameters used to generate the baseline predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4210788\n",
      "Total: 4.21M, Trainable: 4.21M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4210788, 4210788)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = hvatnet.Config(n_electrodes=8, n_channels_out=20,\n",
    "                            n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                            n_filters=128, kernel_size=3,\n",
    "                            strides=(2, 2, 2), dilation=2, \n",
    "                            small_strides = (2, 2))\n",
    "model = hvatnet.HVATNetv3(model_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the predictions are downsampled at 25Hz from the data originally recorded at 200Hz. The `hvatnet` model used here, automatically and correctly downsamples the data during predictions. Make sure that your model's oputput is also downsampled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8, 256), Y shape: (20, 32)\n",
      "Predictions shape: (20, 32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_dataset[0]\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "Y_hat = model(torch.tensor(X).unsqueeze(0)).squeeze().detach().numpy()\n",
    "\n",
    "print(f\"Predictions shape: {Y_hat.shape}\")\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains the baseline model using training code defined in `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed initialization of scheduler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/scratch2/nkudryas/micromamba/envs/bci-kaggle/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/envs/bld/conda-bld/pytorch-select_1719512383855/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/scratch2/nkudryas/micromamba/envs/bci-kaggle/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/envs/bld/conda-bld/pytorch-select_1719512383855/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "overall_steps 150: 0.29736852645874023\n",
      "val loss: 0.3231438994407654\n",
      "saved model:  step_150_loss_0.3231.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 300: 0.25781744718551636\n",
      "val loss: 0.3329932987689972\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 450: 0.2753017842769623\n",
      "val loss: 0.32286664843559265\n",
      "saved model:  step_450_loss_0.3229.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 600: 0.24553827941417694\n",
      "val loss: 0.3519432544708252\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 750: 0.2618802487850189\n",
      "val loss: 0.309486448764801\n",
      "saved model:  step_750_loss_0.3095.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 900: 0.24953725934028625\n",
      "val loss: 0.32483574748039246\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1050: 0.25269415974617004\n",
      "val loss: 0.34946566820144653\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1200: 0.24876795709133148\n",
      "val loss: 0.3090115189552307\n",
      "saved model:  step_1200_loss_0.3090.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1350: 0.25378578901290894\n",
      "val loss: 0.3141574263572693\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1500: 0.22364354133605957\n",
      "val loss: 0.3109073042869568\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1650: 0.2546257972717285\n",
      "val loss: 0.3283170759677887\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1800: 0.2629980146884918\n",
      "val loss: 0.3017239272594452\n",
      "saved model:  step_1800_loss_0.3017.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1950: 0.21416020393371582\n",
      "val loss: 0.30443280935287476\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2100: 0.2767476737499237\n",
      "val loss: 0.3223378360271454\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2250: 0.2445138692855835\n",
      "val loss: 0.3006402850151062\n",
      "saved model:  step_2250_loss_0.3006.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2400: 0.27222108840942383\n",
      "val loss: 0.32069823145866394\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2550: 0.22605185210704803\n",
      "val loss: 0.34346115589141846\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2700: 0.2612375319004059\n",
      "val loss: 0.2953220307826996\n",
      "saved model:  step_2700_loss_0.2953.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2850: 0.21265380084514618\n",
      "val loss: 0.29173174500465393\n",
      "saved model:  step_2850_loss_0.2917.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3000: 0.21886205673217773\n",
      "val loss: 0.2766440510749817\n",
      "saved model:  step_3000_loss_0.2766.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3150: 0.23545943200588226\n",
      "val loss: 0.276885449886322\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3300: 0.22516094148159027\n",
      "val loss: 0.2819821834564209\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3450: 0.23576107621192932\n",
      "val loss: 0.2869633734226227\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3600: 0.2169388383626938\n",
      "val loss: 0.2807248830795288\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3750: 0.20871232450008392\n",
      "val loss: 0.2967377305030823\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3900: 0.2712474465370178\n",
      "val loss: 0.28820371627807617\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4050: 0.24099169671535492\n",
      "val loss: 0.2822190821170807\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4200: 0.22386221587657928\n",
      "val loss: 0.3308179974555969\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4350: 0.2170056402683258\n",
      "val loss: 0.2857514023780823\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4500: 0.20897188782691956\n",
      "val loss: 0.2990470826625824\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4650: 0.20620694756507874\n",
      "val loss: 0.2770005166530609\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4800: 0.21941912174224854\n",
      "val loss: 0.2810235619544983\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4950: 0.2176082581281662\n",
      "val loss: 0.30289211869239807\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5100: 0.21928949654102325\n",
      "val loss: 0.32623496651649475\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5250: 0.21835415065288544\n",
      "val loss: 0.2734752297401428\n",
      "saved model:  step_5250_loss_0.2735.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5400: 0.2434466928243637\n",
      "val loss: 0.28186729550361633\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5550: 0.19180002808570862\n",
      "val loss: 0.28872150182724\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5700: 0.20644526183605194\n",
      "val loss: 0.28553706407546997\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5850: 0.2757202684879303\n",
      "val loss: 0.3284548819065094\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6000: 0.2404925674200058\n",
      "val loss: 0.30460017919540405\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6150: 0.2827282249927521\n",
      "val loss: 0.3990076780319214\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6300: 0.2424713373184204\n",
      "val loss: 0.34694600105285645\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6450: 0.25895339250564575\n",
      "val loss: 0.38827621936798096\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6600: 0.24929820001125336\n",
      "val loss: 0.35812121629714966\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6750: 0.30277395248413086\n",
      "val loss: 0.3470062017440796\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6900: 0.2523697018623352\n",
      "val loss: 0.29628923535346985\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7050: 0.2621990144252777\n",
      "val loss: 0.3201652765274048\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7200: 0.278557151556015\n",
      "val loss: 0.3165314793586731\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7350: 0.2689269483089447\n",
      "val loss: 0.31587520241737366\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7500: 0.2743944823741913\n",
      "val loss: 0.3179694414138794\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7650: 0.268528550863266\n",
      "val loss: 0.39225608110427856\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7800: 0.2516236901283264\n",
      "val loss: 0.3326556980609894\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7950: 0.2613978981971741\n",
      "val loss: 0.3323241174221039\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8100: 0.2645534873008728\n",
      "val loss: 0.33553755283355713\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8250: 0.29207977652549744\n",
      "val loss: 0.3485887348651886\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8400: 0.2608226239681244\n",
      "val loss: 0.30649593472480774\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8550: 0.27683526277542114\n",
      "val loss: 0.30503541231155396\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8700: 0.23112045228481293\n",
      "val loss: 0.35356542468070984\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8850: 0.32528018951416016\n",
      "val loss: 0.31374937295913696\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9000: 0.2772601246833801\n",
      "val loss: 0.3488183915615082\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9150: 0.29688844084739685\n",
      "val loss: 0.3525362014770508\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9300: 0.27067074179649353\n",
      "val loss: 0.333761602640152\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9450: 0.2670263648033142\n",
      "val loss: 0.35083889961242676\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9600: 0.22963254153728485\n",
      "val loss: 0.31600627303123474\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9750: 0.32104116678237915\n",
      "val loss: 0.33387213945388794\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9900: 0.3250218331813812\n",
      "val loss: 0.3540746867656708\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10050: 0.29612308740615845\n",
      "val loss: 0.35124656558036804\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10200: 0.38856974244117737\n",
      "val loss: 0.3533059358596802\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10350: 0.3244914710521698\n",
      "val loss: 0.3496665954589844\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10500: 0.2797427177429199\n",
      "val loss: 0.34416863322257996\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10650: 0.3326447010040283\n",
      "val loss: 0.3172388970851898\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10800: 0.275961697101593\n",
      "val loss: 0.351720929145813\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10950: 0.2982286512851715\n",
      "val loss: 0.3536222279071808\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11100: 0.28417301177978516\n",
      "val loss: 0.31896379590034485\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11250: 0.36694368720054626\n",
      "val loss: 0.4068891108036041\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11400: 0.28604692220687866\n",
      "val loss: 0.32139888405799866\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11550: 0.2802509367465973\n",
      "val loss: 0.3442504107952118\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11700: 0.25918227434158325\n",
      "val loss: 0.3210188150405884\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11850: 0.39636102318763733\n",
      "val loss: 0.4299636483192444\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12000: 0.3648490905761719\n",
      "val loss: 0.32695096731185913\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12150: 0.37282320857048035\n",
      "val loss: 0.37596550583839417\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12300: 0.28195229172706604\n",
      "val loss: 0.3580531179904938\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12450: 0.3561757504940033\n",
      "val loss: 0.3509765863418579\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12600: 0.33880090713500977\n",
      "val loss: 0.32104310393333435\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12750: 0.3394128978252411\n",
      "val loss: 0.368922621011734\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12900: 0.30901962518692017\n",
      "val loss: 0.3435118794441223\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13050: 0.2926514744758606\n",
      "val loss: 0.3601617217063904\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13200: 0.2521445155143738\n",
      "val loss: 0.3526246249675751\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13350: 0.31519052386283875\n",
      "val loss: 0.3277008831501007\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13500: 0.3007412850856781\n",
      "val loss: 0.3457425832748413\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13650: 0.3322009742259979\n",
      "val loss: 0.3592298626899719\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13800: 0.34749338030815125\n",
      "val loss: 0.3317873477935791\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13950: 0.3009650409221649\n",
      "val loss: 0.3227868676185608\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14100: 0.2789364457130432\n",
      "val loss: 0.3442223072052002\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14250: 0.32581496238708496\n",
      "val loss: 0.3379635810852051\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14400: 0.31676939129829407\n",
      "val loss: 0.33899828791618347\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14550: 0.3204067051410675\n",
      "val loss: 0.3478248417377472\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14700: 0.31839311122894287\n",
      "val loss: 0.3820059597492218\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14850: 0.33306190371513367\n",
      "val loss: 0.346432626247406\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15000: 0.295274943113327\n",
      "val loss: 0.34164291620254517\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15150: 0.34721270203590393\n",
      "val loss: 0.4030212461948395\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15300: 0.2916274070739746\n",
      "val loss: 0.34942811727523804\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15450: 0.373849481344223\n",
      "val loss: 0.3540312945842743\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15600: 0.36609479784965515\n",
      "val loss: 0.3402702808380127\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15750: 0.29137834906578064\n",
      "val loss: 0.3802706301212311\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15900: 0.3040999472141266\n",
      "val loss: 0.32776233553886414\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16050: 0.30193862318992615\n",
      "val loss: 0.34061649441719055\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16200: 0.2820684611797333\n",
      "val loss: 0.36619752645492554\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16350: 0.29869481921195984\n",
      "val loss: 0.4021233916282654\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16500: 0.2890187203884125\n",
      "val loss: 0.3435300588607788\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16650: 0.30001458525657654\n",
      "val loss: 0.40844714641571045\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16800: 0.29157838225364685\n",
      "val loss: 0.3452691435813904\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16950: 0.3146345615386963\n",
      "val loss: 0.3380796015262604\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17100: 0.3732185363769531\n",
      "val loss: 0.39380645751953125\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17250: 0.2997649312019348\n",
      "val loss: 0.36278289556503296\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17400: 0.35537829995155334\n",
      "val loss: 0.38018566370010376\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17550: 0.3193311095237732\n",
      "val loss: 0.366482675075531\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17700: 0.3112526834011078\n",
      "val loss: 0.3821829855442047\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17850: 0.3107786774635315\n",
      "val loss: 0.3339552581310272\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18000: 0.2934284806251526\n",
      "val loss: 0.36718952655792236\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18150: 0.37243953347206116\n",
      "val loss: 0.4083257019519806\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18300: 0.35981830954551697\n",
      "val loss: 0.37927332520484924\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18450: 0.37724682688713074\n",
      "val loss: 0.3969910442829132\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18600: 0.3592820167541504\n",
      "val loss: 0.3625577986240387\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18750: 0.2990903854370117\n",
      "val loss: 0.348645955324173\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18900: 0.35118457674980164\n",
      "val loss: 0.40809112787246704\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19050: 0.32341596484184265\n",
      "val loss: 0.3374291658401489\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19200: 0.3555787205696106\n",
      "val loss: 0.35550859570503235\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19350: 0.8308743834495544\n",
      "val loss: 0.6472679376602173\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19500: 0.47631388902664185\n",
      "val loss: 0.41328468918800354\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19650: 0.34702521562576294\n",
      "val loss: 0.3598209023475647\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19800: 0.6976598501205444\n",
      "val loss: 1.192604422569275\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19950: 0.3395484983921051\n",
      "val loss: 0.41264304518699646\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20100: 0.35089054703712463\n",
      "val loss: 0.7066007852554321\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20250: 0.35480865836143494\n",
      "val loss: 0.361326664686203\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20400: 0.36980879306793213\n",
      "val loss: 0.5944276452064514\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20550: 0.3226162791252136\n",
      "val loss: 0.34229323267936707\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20700: 0.3512578308582306\n",
      "val loss: 0.3459644317626953\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20850: 0.38694873452186584\n",
      "val loss: 0.4160173535346985\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21000: 0.3902834355831146\n",
      "val loss: 0.485026091337204\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21150: 0.3970720171928406\n",
      "val loss: 0.4500022232532501\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21300: 0.4070620536804199\n",
      "val loss: 0.4596065282821655\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21450: 0.4736809730529785\n",
      "val loss: 0.5135213732719421\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21600: 0.34055644273757935\n",
      "val loss: 0.367605984210968\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21750: 0.33794543147087097\n",
      "val loss: 0.3660554885864258\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21900: 0.3691778779029846\n",
      "val loss: 0.3520481288433075\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22050: 0.3922952711582184\n",
      "val loss: 0.34392547607421875\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22200: 0.33550700545310974\n",
      "val loss: 0.3514546751976013\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22350: 0.3456602990627289\n",
      "val loss: 0.34495964646339417\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22500: 0.3526392877101898\n",
      "val loss: 0.35893514752388\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22650: 0.34996461868286133\n",
      "val loss: 0.3597218990325928\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22800: 0.3478088080883026\n",
      "val loss: 0.3833695948123932\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22950: 0.3134370446205139\n",
      "val loss: 0.3473879396915436\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23100: 0.3664810061454773\n",
      "val loss: 0.38207054138183594\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23250: 0.31159380078315735\n",
      "val loss: 0.3511717915534973\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23400: 0.35211965441703796\n",
      "val loss: 0.39204803109169006\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23550: 0.4189753532409668\n",
      "val loss: 0.4150082767009735\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23700: 0.30916646122932434\n",
      "val loss: 0.36449065804481506\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23850: 0.3001367151737213\n",
      "val loss: 0.3817349076271057\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24000: 0.32508358359336853\n",
      "val loss: 0.3538872003555298\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24150: 0.34196937084198\n",
      "val loss: 0.40621787309646606\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24300: 0.36925825476646423\n",
      "val loss: 0.4003174901008606\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24450: 0.3282027542591095\n",
      "val loss: 0.4618217349052429\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24600: 0.3192148208618164\n",
      "val loss: 0.4014052748680115\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24750: 0.33255481719970703\n",
      "val loss: 0.4083264172077179\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24900: 0.3269382119178772\n",
      "val loss: 0.36549949645996094\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25050: 0.29053351283073425\n",
      "val loss: 0.3382379114627838\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25200: 0.39102211594581604\n",
      "val loss: 0.3880442678928375\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25350: 0.3540878891944885\n",
      "val loss: 0.3761080503463745\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25500: 0.3721453845500946\n",
      "val loss: 0.4343928098678589\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25650: 0.35097718238830566\n",
      "val loss: 0.3634682893753052\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25800: 0.3241819739341736\n",
      "val loss: 0.4630267322063446\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25950: 0.3474524915218353\n",
      "val loss: 0.39801403880119324\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26100: 0.33237987756729126\n",
      "val loss: 0.4767317473888397\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26250: 0.34625813364982605\n",
      "val loss: 0.3917202651500702\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26400: 0.36542293429374695\n",
      "val loss: 0.40688443183898926\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26550: 0.39665746688842773\n",
      "val loss: 0.4077224135398865\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26700: 0.3176855146884918\n",
      "val loss: 0.3516138792037964\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26850: 0.36879387497901917\n",
      "val loss: 0.38626351952552795\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27000: 0.7616018652915955\n",
      "val loss: 1.0807548761367798\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27150: 0.38893017172813416\n",
      "val loss: 0.3862922191619873\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27300: 0.3308870494365692\n",
      "val loss: 0.3659485876560211\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27450: 0.3433930575847626\n",
      "val loss: 0.36027565598487854\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27600: 0.3583696484565735\n",
      "val loss: 0.3674335479736328\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27750: 0.36257418990135193\n",
      "val loss: 0.363185316324234\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27900: 0.30225786566734314\n",
      "val loss: 0.32678601145744324\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28050: 0.35249945521354675\n",
      "val loss: 0.398296594619751\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28200: 0.33107757568359375\n",
      "val loss: 0.40194013714790344\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28350: 0.4249233305454254\n",
      "val loss: 0.42711594700813293\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28500: 0.3733106553554535\n",
      "val loss: 0.36004185676574707\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28650: 0.44484320282936096\n",
      "val loss: 0.4465336501598358\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28800: 0.3687463700771332\n",
      "val loss: 0.4241313636302948\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28950: 0.30337968468666077\n",
      "val loss: 0.35318005084991455\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29100: 0.3308866620063782\n",
      "val loss: 0.3417913019657135\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29250: 0.3518490493297577\n",
      "val loss: 0.40573906898498535\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29400: 0.2937373220920563\n",
      "val loss: 0.3383212089538574\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29550: 0.32481512427330017\n",
      "val loss: 0.3800901770591736\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29700: 0.383286714553833\n",
      "val loss: 0.3792409598827362\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29850: 0.3044855296611786\n",
      "val loss: 0.3309929072856903\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30000: 0.3408527076244354\n",
      "val loss: 0.3509806990623474\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30150: 0.33281955122947693\n",
      "val loss: 0.3398306369781494\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30300: 0.40897026658058167\n",
      "val loss: 0.38283538818359375\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30450: 0.33634573221206665\n",
      "val loss: 0.372903972864151\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30600: 0.3166501224040985\n",
      "val loss: 0.34693506360054016\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30750: 0.32817941904067993\n",
      "val loss: 0.4016377031803131\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30900: 0.33785438537597656\n",
      "val loss: 0.40575578808784485\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31050: 0.30992814898490906\n",
      "val loss: 0.3448115885257721\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31200: 0.4330197274684906\n",
      "val loss: 0.3908452093601227\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31350: 0.5261709690093994\n",
      "val loss: 0.47912657260894775\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31500: 0.3292369842529297\n",
      "val loss: 0.32840701937675476\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31650: 0.3865116536617279\n",
      "val loss: 0.37474551796913147\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31800: 0.36791667342185974\n",
      "val loss: 0.4105738401412964\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31950: 0.3165111243724823\n",
      "val loss: 0.32513031363487244\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32100: 0.3206680417060852\n",
      "val loss: 0.34506866335868835\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32250: 0.3169899880886078\n",
      "val loss: 0.3231273889541626\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32400: 0.28360095620155334\n",
      "val loss: 0.31921908259391785\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32550: 0.35701748728752136\n",
      "val loss: 0.33432552218437195\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32700: 0.3044176697731018\n",
      "val loss: 0.345255583524704\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32850: 0.30455952882766724\n",
      "val loss: 0.33796435594558716\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33000: 0.435612291097641\n",
      "val loss: 0.3951074182987213\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33150: 0.3431054651737213\n",
      "val loss: 0.3394814133644104\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33300: 0.5943889021873474\n",
      "val loss: 0.49747008085250854\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33450: 1.2008885145187378\n",
      "val loss: 0.8069377541542053\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33600: 0.3429296016693115\n",
      "val loss: 0.3607495427131653\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33750: 0.33087271451950073\n",
      "val loss: 0.36039984226226807\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33900: 0.2971508204936981\n",
      "val loss: 0.3440621495246887\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34050: 0.47051382064819336\n",
      "val loss: 0.4459398090839386\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34200: 0.35269102454185486\n",
      "val loss: 0.35524722933769226\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34350: 0.3265784680843353\n",
      "val loss: 0.3601381480693817\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34500: 0.3173312842845917\n",
      "val loss: 0.37165382504463196\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34650: 0.3675808012485504\n",
      "val loss: 0.35403817892074585\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34800: 0.34583982825279236\n",
      "val loss: 0.3225836157798767\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34950: 0.34555384516716003\n",
      "val loss: 0.3421476185321808\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35100: 0.3809352219104767\n",
      "val loss: 0.33237266540527344\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35250: 0.31789514422416687\n",
      "val loss: 0.3369598090648651\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35400: 0.3595999777317047\n",
      "val loss: 0.37056389451026917\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35550: 0.3436153829097748\n",
      "val loss: 0.34197816252708435\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35700: 0.4238869249820709\n",
      "val loss: 0.3536781072616577\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35850: 0.3229645788669586\n",
      "val loss: 0.33763593435287476\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36000: 0.3440776765346527\n",
      "val loss: 0.3430057168006897\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36150: 0.3111542761325836\n",
      "val loss: 0.33933520317077637\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36300: 0.4113520681858063\n",
      "val loss: 0.40578755736351013\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36450: 0.3432098925113678\n",
      "val loss: 0.36217427253723145\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36600: 0.34245672821998596\n",
      "val loss: 0.34128546714782715\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36750: 0.3331528306007385\n",
      "val loss: 0.3365291953086853\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36900: 0.3035702407360077\n",
      "val loss: 0.3394644260406494\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37050: 0.33801528811454773\n",
      "val loss: 0.35792917013168335\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37200: 0.3315608501434326\n",
      "val loss: 0.32307395339012146\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37350: 0.34490785002708435\n",
      "val loss: 0.33833372592926025\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37500: 0.3414599597454071\n",
      "val loss: 0.3285704255104065\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37650: 0.3396551311016083\n",
      "val loss: 0.3296804428100586\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37800: 0.32853326201438904\n",
      "val loss: 0.3350636959075928\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37950: 0.3626417815685272\n",
      "val loss: 0.33236995339393616\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38100: 0.3337714374065399\n",
      "val loss: 0.3364231586456299\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38250: 0.3151489794254303\n",
      "val loss: 0.33020272850990295\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38400: 0.3490895628929138\n",
      "val loss: 0.3282908499240875\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38550: 0.32141971588134766\n",
      "val loss: 0.32686659693717957\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38700: 0.32235273718833923\n",
      "val loss: 0.33754438161849976\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38850: 0.29577940702438354\n",
      "val loss: 0.3727087676525116\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39000: 0.31298479437828064\n",
      "val loss: 0.3831716477870941\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39150: 0.316524475812912\n",
      "val loss: 0.3245411813259125\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39300: 0.33847007155418396\n",
      "val loss: 0.3536902368068695\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39450: 0.3179035484790802\n",
      "val loss: 0.3444807529449463\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39600: 0.3296372890472412\n",
      "val loss: 0.33772557973861694\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39750: 0.31139668822288513\n",
      "val loss: 0.3349786698818207\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39900: 0.3261658847332001\n",
      "val loss: 0.346270352602005\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40050: 0.3121676445007324\n",
      "val loss: 0.3533484637737274\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40200: 0.30784913897514343\n",
      "val loss: 0.34008294343948364\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40350: 0.3211589455604553\n",
      "val loss: 0.3610806465148926\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40500: 0.30772876739501953\n",
      "val loss: 0.3289856016635895\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40650: 0.3095615804195404\n",
      "val loss: 0.3484118580818176\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40800: 0.4418795108795166\n",
      "val loss: 0.42928367853164673\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40950: 0.31767264008522034\n",
      "val loss: 0.32340797781944275\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41100: 0.31330403685569763\n",
      "val loss: 0.3313666582107544\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41250: 0.3128162920475006\n",
      "val loss: 0.3295401930809021\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41400: 0.30141544342041016\n",
      "val loss: 0.32923948764801025\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41550: 0.3298969268798828\n",
      "val loss: 0.33300909399986267\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41700: 0.3343920409679413\n",
      "val loss: 0.3385404646396637\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41850: 0.30941876769065857\n",
      "val loss: 0.33084815740585327\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42000: 0.2998308837413788\n",
      "val loss: 0.3277646601200104\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42150: 0.328984797000885\n",
      "val loss: 0.36022719740867615\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42300: 0.32623258233070374\n",
      "val loss: 0.31313765048980713\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42450: 0.3157660663127899\n",
      "val loss: 0.32306718826293945\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42600: 0.3063974976539612\n",
      "val loss: 0.33445748686790466\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42750: 0.3109874725341797\n",
      "val loss: 0.3667522072792053\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42900: 0.2945576310157776\n",
      "val loss: 0.32455554604530334\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43050: 0.3075443506240845\n",
      "val loss: 0.34534531831741333\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43200: 0.3162650763988495\n",
      "val loss: 0.33131298422813416\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43350: 0.33269861340522766\n",
      "val loss: 0.3340022563934326\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43500: 0.2883100211620331\n",
      "val loss: 0.3267616629600525\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43650: 0.29014039039611816\n",
      "val loss: 0.32901737093925476\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43800: 0.2639634907245636\n",
      "val loss: 0.3253459334373474\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43950: 0.2890467047691345\n",
      "val loss: 0.3245289921760559\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44100: 0.337711900472641\n",
      "val loss: 0.3252841532230377\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44250: 0.3399364650249481\n",
      "val loss: 0.329111248254776\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44400: 0.31960442662239075\n",
      "val loss: 0.3238498270511627\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44550: 0.3264870345592499\n",
      "val loss: 0.3304101526737213\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44700: 0.30418217182159424\n",
      "val loss: 0.3304469883441925\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44850: 0.3201207220554352\n",
      "val loss: 0.3292073607444763\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45000: 0.3406355381011963\n",
      "val loss: 0.3361612558364868\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45150: 0.29303619265556335\n",
      "val loss: 0.3315664529800415\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45300: 0.2646314799785614\n",
      "val loss: 0.32912203669548035\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45450: 0.3064890503883362\n",
      "val loss: 0.3311555087566376\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45600: 0.30475184321403503\n",
      "val loss: 0.3376798629760742\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45750: 0.3022182881832123\n",
      "val loss: 0.3275929391384125\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45900: 0.3019707500934601\n",
      "val loss: 0.3297991454601288\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46050: 0.2950630784034729\n",
      "val loss: 0.3568085730075836\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46200: 0.2902834713459015\n",
      "val loss: 0.32464689016342163\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46350: 0.28121325373649597\n",
      "val loss: 0.3349447250366211\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46500: 0.30650192499160767\n",
      "val loss: 0.32771429419517517\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46650: 0.2819307744503021\n",
      "val loss: 0.3553411066532135\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46800: 0.2945358455181122\n",
      "val loss: 0.3279574513435364\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46950: 0.2880747318267822\n",
      "val loss: 0.32609185576438904\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47100: 0.302206814289093\n",
      "val loss: 0.3247925937175751\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47250: 0.3105265200138092\n",
      "val loss: 0.3717573881149292\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47400: 0.2821849584579468\n",
      "val loss: 0.3223108947277069\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47550: 0.3323913812637329\n",
      "val loss: 0.33959609270095825\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47700: 0.2992599308490753\n",
      "val loss: 0.32654690742492676\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47850: 0.2957073152065277\n",
      "val loss: 0.35570213198661804\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48000: 0.302176833152771\n",
      "val loss: 0.33441898226737976\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48150: 0.29633843898773193\n",
      "val loss: 0.3941354751586914\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48300: 0.3115670382976532\n",
      "val loss: 0.3536303639411926\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48450: 0.3007287383079529\n",
      "val loss: 0.3706118166446686\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48600: 0.293021559715271\n",
      "val loss: 0.3277747631072998\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48750: 0.2912282347679138\n",
      "val loss: 0.32515737414360046\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48900: 0.31489625573158264\n",
      "val loss: 0.34226423501968384\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49050: 0.3334578573703766\n",
      "val loss: 0.3413752317428589\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49200: 0.2996719479560852\n",
      "val loss: 0.3504292964935303\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49350: 0.271038293838501\n",
      "val loss: 0.3371632397174835\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49500: 0.2966158986091614\n",
      "val loss: 0.3374154269695282\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49650: 0.300912082195282\n",
      "val loss: 0.3374960422515869\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49800: 0.28628769516944885\n",
      "val loss: 0.3516135811805725\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49950: 0.30642539262771606\n",
      "val loss: 0.34213683009147644\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50100: 0.31815436482429504\n",
      "val loss: 0.4376075863838196\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50250: 0.32310420274734497\n",
      "val loss: 0.3930496871471405\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50400: 0.31680306792259216\n",
      "val loss: 0.3478504419326782\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50550: 0.3061242997646332\n",
      "val loss: 0.3411872982978821\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50700: 0.2944962978363037\n",
      "val loss: 0.3219875395298004\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50850: 0.2955709397792816\n",
      "val loss: 0.3365737497806549\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51000: 0.3237597346305847\n",
      "val loss: 0.345247745513916\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51150: 0.30078235268592834\n",
      "val loss: 0.34211719036102295\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51300: 0.29305997490882874\n",
      "val loss: 0.3364655077457428\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51450: 0.2740018963813782\n",
      "val loss: 0.3350967764854431\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51600: 0.3231607973575592\n",
      "val loss: 0.3378923237323761\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51750: 0.3199577033519745\n",
      "val loss: 0.34410396218299866\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51900: 0.3076840341091156\n",
      "val loss: 0.35430601239204407\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52050: 0.28683724999427795\n",
      "val loss: 0.3713051974773407\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52200: 0.3417222201824188\n",
      "val loss: 0.3633716404438019\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52350: 0.2807741165161133\n",
      "val loss: 0.3188992440700531\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52500: 0.2980073094367981\n",
      "val loss: 0.32948097586631775\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52650: 0.284611314535141\n",
      "val loss: 0.34470999240875244\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52800: 0.3046204149723053\n",
      "val loss: 0.3391086757183075\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52950: 0.2952059507369995\n",
      "val loss: 0.35390135645866394\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53100: 0.27724358439445496\n",
      "val loss: 0.34521782398223877\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53250: 0.2728915214538574\n",
      "val loss: 0.3279949426651001\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53400: 0.30037403106689453\n",
      "val loss: 0.3208748996257782\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53550: 0.29976147413253784\n",
      "val loss: 0.3519899547100067\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53700: 0.3163245916366577\n",
      "val loss: 0.3310597538948059\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53850: 0.2680054306983948\n",
      "val loss: 0.3416104316711426\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54000: 0.30928540229797363\n",
      "val loss: 0.34404125809669495\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54150: 0.2961924076080322\n",
      "val loss: 0.3490005135536194\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54300: 0.31029340624809265\n",
      "val loss: 0.35075312852859497\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54450: 0.32840070128440857\n",
      "val loss: 0.3311150074005127\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54600: 0.26578065752983093\n",
      "val loss: 0.3537178039550781\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54750: 0.2796991467475891\n",
      "val loss: 0.33464929461479187\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54900: 0.2762733995914459\n",
      "val loss: 0.32578468322753906\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55050: 0.2850598990917206\n",
      "val loss: 0.33700957894325256\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55200: 0.27857694029808044\n",
      "val loss: 0.32048413157463074\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55350: 0.2901516556739807\n",
      "val loss: 0.34891071915626526\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55500: 0.30449652671813965\n",
      "val loss: 0.35094934701919556\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55650: 0.3034001886844635\n",
      "val loss: 0.34114280343055725\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55800: 0.3501346707344055\n",
      "val loss: 0.3495519161224365\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55950: 0.3134966790676117\n",
      "val loss: 0.3253311216831207\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56100: 0.30772343277931213\n",
      "val loss: 0.3634937107563019\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56250: 0.31362080574035645\n",
      "val loss: 0.3246503472328186\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56400: 0.29380419850349426\n",
      "val loss: 0.33392634987831116\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56550: 0.29967793822288513\n",
      "val loss: 0.33879587054252625\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56700: 0.2946854531764984\n",
      "val loss: 0.3469727635383606\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56850: 0.29302871227264404\n",
      "val loss: 0.3478025794029236\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57000: 0.2977370321750641\n",
      "val loss: 0.3376692831516266\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57150: 0.2800643742084503\n",
      "val loss: 0.34344667196273804\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57300: 0.3158908486366272\n",
      "val loss: 0.33018729090690613\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57450: 0.2795723080635071\n",
      "val loss: 0.3401675820350647\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57600: 0.2827936112880707\n",
      "val loss: 0.33060675859451294\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57750: 0.30473336577415466\n",
      "val loss: 0.37163183093070984\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57900: 0.296957403421402\n",
      "val loss: 0.33346042037010193\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58050: 0.330696165561676\n",
      "val loss: 0.319805771112442\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58200: 0.2913966774940491\n",
      "val loss: 0.3319304585456848\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58350: 0.27235227823257446\n",
      "val loss: 0.33660900592803955\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58500: 0.27580639719963074\n",
      "val loss: 0.32283204793930054\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58650: 0.30367788672447205\n",
      "val loss: 0.3549978733062744\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58800: 0.28262218832969666\n",
      "val loss: 0.3275870978832245\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58950: 0.30171889066696167\n",
      "val loss: 0.34205928444862366\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59100: 0.29801133275032043\n",
      "val loss: 0.3461506962776184\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59250: 0.31672993302345276\n",
      "val loss: 0.3239820897579193\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59400: 0.2971709668636322\n",
      "val loss: 0.3472477197647095\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59550: 0.33129677176475525\n",
      "val loss: 0.3421421945095062\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59700: 0.30700814723968506\n",
      "val loss: 0.3194499909877777\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59850: 0.32837429642677307\n",
      "val loss: 0.3554372191429138\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60000: 0.30509576201438904\n",
      "val loss: 0.33469507098197937\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60150: 0.2986021041870117\n",
      "val loss: 0.33650779724121094\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60300: 0.27661848068237305\n",
      "val loss: 0.3245970904827118\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60450: 0.3012835681438446\n",
      "val loss: 0.33203834295272827\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60600: 0.29344120621681213\n",
      "val loss: 0.3556342124938965\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60750: 0.30204111337661743\n",
      "val loss: 0.32172781229019165\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60900: 0.28796911239624023\n",
      "val loss: 0.33375483751296997\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61050: 0.2718639075756073\n",
      "val loss: 0.33106300234794617\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61200: 0.2915706932544708\n",
      "val loss: 0.325833797454834\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61350: 0.2967233657836914\n",
      "val loss: 0.3276432156562805\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61500: 0.28110846877098083\n",
      "val loss: 0.33286359906196594\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61650: 0.30129295587539673\n",
      "val loss: 0.36431655287742615\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61800: 0.3090984523296356\n",
      "val loss: 0.361027330160141\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61950: 0.2807054817676544\n",
      "val loss: 0.3390909433364868\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62100: 0.28184375166893005\n",
      "val loss: 0.3366261124610901\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62250: 0.30800479650497437\n",
      "val loss: 0.3256928622722626\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62400: 0.28465065360069275\n",
      "val loss: 0.34560510516166687\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62550: 0.3084770143032074\n",
      "val loss: 0.34834742546081543\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62700: 0.2858692705631256\n",
      "val loss: 0.322925865650177\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62850: 0.3038461208343506\n",
      "val loss: 0.3441908061504364\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63000: 0.3037968575954437\n",
      "val loss: 0.3259762227535248\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63150: 0.428090900182724\n",
      "val loss: 0.46575215458869934\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63300: 0.28697580099105835\n",
      "val loss: 0.3381947875022888\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63450: 0.29914960265159607\n",
      "val loss: 0.32290756702423096\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63600: 0.3164011538028717\n",
      "val loss: 0.3163480758666992\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63750: 0.28557726740837097\n",
      "val loss: 0.33017227053642273\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63900: 0.28927141427993774\n",
      "val loss: 0.3330862522125244\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64050: 0.28728601336479187\n",
      "val loss: 0.31906187534332275\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64200: 0.3131377696990967\n",
      "val loss: 0.3489663898944855\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64350: 0.29491838812828064\n",
      "val loss: 0.33784762024879456\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64500: 0.28258344531059265\n",
      "val loss: 0.3415336310863495\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64650: 0.27086341381073\n",
      "val loss: 0.331796795129776\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64800: 0.3012616038322449\n",
      "val loss: 0.33295372128486633\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64950: 0.2725277543067932\n",
      "val loss: 0.33270683884620667\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65100: 0.36090290546417236\n",
      "val loss: 0.3576740026473999\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65250: 0.295419842004776\n",
      "val loss: 0.32172951102256775\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65400: 0.3470397889614105\n",
      "val loss: 0.3540436327457428\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65550: 0.3495982587337494\n",
      "val loss: 0.34742918610572815\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65700: 0.28959354758262634\n",
      "val loss: 0.3199010491371155\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65850: 0.3113977611064911\n",
      "val loss: 0.311987042427063\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66000: 0.29717546701431274\n",
      "val loss: 0.30752116441726685\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66150: 0.35263893008232117\n",
      "val loss: 0.3588179349899292\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66300: 0.29894956946372986\n",
      "val loss: 0.3074309527873993\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66450: 0.2622654438018799\n",
      "val loss: 0.3164604604244232\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66600: 0.31006941199302673\n",
      "val loss: 0.3471831977367401\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66750: 0.3180306851863861\n",
      "val loss: 0.32668337225914\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66900: 0.29482904076576233\n",
      "val loss: 0.3227737843990326\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67050: 0.2789679169654846\n",
      "val loss: 0.3259316384792328\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67200: 0.28626808524131775\n",
      "val loss: 0.3127001225948334\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67350: 0.2945910394191742\n",
      "val loss: 0.32494062185287476\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67500: 0.2863500118255615\n",
      "val loss: 0.3233790099620819\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67650: 0.2952432334423065\n",
      "val loss: 0.31248342990875244\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67800: 0.3336935341358185\n",
      "val loss: 0.3407636880874634\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67950: 0.2781336307525635\n",
      "val loss: 0.32513388991355896\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68100: 0.29784899950027466\n",
      "val loss: 0.33747732639312744\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68250: 0.27340957522392273\n",
      "val loss: 0.3290463984012604\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68400: 0.2844878137111664\n",
      "val loss: 0.3229229748249054\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68550: 0.2829664647579193\n",
      "val loss: 0.3199300765991211\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68700: 0.3004635274410248\n",
      "val loss: 0.3341493308544159\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68850: 0.2843746244907379\n",
      "val loss: 0.3181649446487427\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69000: 0.33076727390289307\n",
      "val loss: 0.3551727533340454\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69150: 0.2785768508911133\n",
      "val loss: 0.339426189661026\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69300: 0.2960864007472992\n",
      "val loss: 0.3402751088142395\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69450: 0.3227108418941498\n",
      "val loss: 0.3132043480873108\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69600: 0.3000779449939728\n",
      "val loss: 0.3285331726074219\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69750: 0.28180748224258423\n",
      "val loss: 0.3278524875640869\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69900: 0.28662753105163574\n",
      "val loss: 0.32201358675956726\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70050: 0.2983451783657074\n",
      "val loss: 0.3324127197265625\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70200: 0.3141936659812927\n",
      "val loss: 0.332426518201828\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70350: 0.2775091528892517\n",
      "val loss: 0.3265937268733978\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70500: 0.30377352237701416\n",
      "val loss: 0.32174137234687805\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70650: 0.3040020167827606\n",
      "val loss: 0.33188220858573914\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70800: 0.2865511476993561\n",
      "val loss: 0.3313243091106415\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70950: 0.28607988357543945\n",
      "val loss: 0.3355032205581665\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71100: 0.3108566999435425\n",
      "val loss: 0.32755813002586365\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71250: 0.30171170830726624\n",
      "val loss: 0.346220463514328\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71400: 0.2708311975002289\n",
      "val loss: 0.33029791712760925\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71550: 0.29914143681526184\n",
      "val loss: 0.33178800344467163\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71700: 0.28020039200782776\n",
      "val loss: 0.32993999123573303\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71850: 0.29340872168540955\n",
      "val loss: 0.32631370425224304\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72000: 0.286298930644989\n",
      "val loss: 0.3305834233760834\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72150: 0.2920461893081665\n",
      "val loss: 0.33619606494903564\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72300: 0.2885732352733612\n",
      "val loss: 0.3205457925796509\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72450: 0.32362085580825806\n",
      "val loss: 0.3564054071903229\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72600: 0.2864936292171478\n",
      "val loss: 0.3361116349697113\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72750: 0.30139783024787903\n",
      "val loss: 0.3329923152923584\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72900: 0.28133735060691833\n",
      "val loss: 0.3300754725933075\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73050: 0.29369398951530457\n",
      "val loss: 0.327461838722229\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73200: 0.31379786133766174\n",
      "val loss: 0.32133302092552185\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73350: 0.30023011565208435\n",
      "val loss: 0.33631786704063416\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73500: 0.28726109862327576\n",
      "val loss: 0.3491012156009674\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73650: 0.2728857696056366\n",
      "val loss: 0.3167864978313446\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73800: 0.274737685918808\n",
      "val loss: 0.3267676532268524\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73950: 0.3262600600719452\n",
      "val loss: 0.3237501084804535\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74100: 0.29272231459617615\n",
      "val loss: 0.3196088373661041\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74250: 0.2817706763744354\n",
      "val loss: 0.3303893208503723\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74400: 0.284125417470932\n",
      "val loss: 0.3537043631076813\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74550: 0.276055246591568\n",
      "val loss: 0.3352837562561035\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74700: 0.2977895736694336\n",
      "val loss: 0.3395085036754608\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74850: 0.3063579499721527\n",
      "val loss: 0.33276858925819397\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75000: 0.2900504767894745\n",
      "val loss: 0.34161531925201416\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75150: 0.2923269271850586\n",
      "val loss: 0.33960577845573425\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75300: 0.34397733211517334\n",
      "val loss: 0.41563791036605835\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75450: 0.2908026874065399\n",
      "val loss: 0.34586724638938904\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75600: 0.30649232864379883\n",
      "val loss: 0.33417317271232605\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75750: 0.3014540374279022\n",
      "val loss: 0.33176541328430176\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75900: 0.2755511701107025\n",
      "val loss: 0.32620713114738464\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76050: 0.273048996925354\n",
      "val loss: 0.3359460234642029\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76200: 0.2888771891593933\n",
      "val loss: 0.3437734544277191\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76350: 0.2821943461894989\n",
      "val loss: 0.35444656014442444\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76500: 0.2728990912437439\n",
      "val loss: 0.32555848360061646\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76650: 0.29075953364372253\n",
      "val loss: 0.31960368156433105\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76800: 0.27716198563575745\n",
      "val loss: 0.33367976546287537\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76950: 0.31098657846450806\n",
      "val loss: 0.40356433391571045\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77100: 0.279741108417511\n",
      "val loss: 0.3302856683731079\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77250: 0.29035425186157227\n",
      "val loss: 0.3312898278236389\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77400: 0.29003071784973145\n",
      "val loss: 0.3469752073287964\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77550: 0.277010977268219\n",
      "val loss: 0.3306274116039276\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77700: 0.26952096819877625\n",
      "val loss: 0.33198246359825134\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77850: 0.29569554328918457\n",
      "val loss: 0.3321896493434906\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78000: 0.27984392642974854\n",
      "val loss: 0.3389895558357239\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78150: 0.3256034851074219\n",
      "val loss: 0.3254241645336151\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78300: 0.2988302409648895\n",
      "val loss: 0.3545016348361969\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78450: 0.26772820949554443\n",
      "val loss: 0.32473286986351013\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78600: 0.30809053778648376\n",
      "val loss: 0.33854764699935913\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78750: 0.26946505904197693\n",
      "val loss: 0.3386414349079132\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78900: 0.2715452015399933\n",
      "val loss: 0.3293877840042114\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79050: 0.28549084067344666\n",
      "val loss: 0.3194250762462616\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79200: 0.2914896011352539\n",
      "val loss: 0.33598005771636963\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79350: 0.28445449471473694\n",
      "val loss: 0.3225807249546051\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79500: 0.30347880721092224\n",
      "val loss: 0.35000869631767273\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79650: 0.27629780769348145\n",
      "val loss: 0.3433128297328949\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79800: 0.3112437427043915\n",
      "val loss: 0.3408436179161072\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79950: 0.27794918417930603\n",
      "val loss: 0.3169635832309723\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80100: 0.28205224871635437\n",
      "val loss: 0.3277786374092102\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80250: 0.2922094762325287\n",
      "val loss: 0.326951265335083\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80400: 0.3095642626285553\n",
      "val loss: 0.33563604950904846\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80550: 0.2556389272212982\n",
      "val loss: 0.32350218296051025\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80700: 0.26660117506980896\n",
      "val loss: 0.32609081268310547\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80850: 0.3303823471069336\n",
      "val loss: 0.3435547947883606\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81000: 0.2690826952457428\n",
      "val loss: 0.3299335539340973\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81150: 0.31066209077835083\n",
      "val loss: 0.3351688086986542\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81300: 0.28918886184692383\n",
      "val loss: 0.33881670236587524\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81450: 0.2992953360080719\n",
      "val loss: 0.34264037013053894\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81600: 0.289833128452301\n",
      "val loss: 0.33200302720069885\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81750: 0.3001645803451538\n",
      "val loss: 0.3306001126766205\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81900: 0.27237173914909363\n",
      "val loss: 0.34521377086639404\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82050: 0.29867056012153625\n",
      "val loss: 0.33372825384140015\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82200: 0.29623106122016907\n",
      "val loss: 0.3221254050731659\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82350: 0.30897101759910583\n",
      "val loss: 0.34470435976982117\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82500: 0.2986408472061157\n",
      "val loss: 0.32899990677833557\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82650: 0.3014502227306366\n",
      "val loss: 0.33631595969200134\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82800: 0.3000434339046478\n",
      "val loss: 0.33495742082595825\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82950: 0.3549833297729492\n",
      "val loss: 0.3861137628555298\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83100: 0.2850073277950287\n",
      "val loss: 0.32694780826568604\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83250: 0.2877945601940155\n",
      "val loss: 0.325035959482193\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83400: 0.28203776478767395\n",
      "val loss: 0.32485976815223694\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83550: 0.2824654281139374\n",
      "val loss: 0.32690075039863586\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83700: 0.3088866174221039\n",
      "val loss: 0.3269568085670471\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83850: 0.2600986063480377\n",
      "val loss: 0.3346700072288513\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84000: 0.2684599459171295\n",
      "val loss: 0.33244964480400085\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84150: 0.28834351897239685\n",
      "val loss: 0.33314821124076843\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84300: 0.29374414682388306\n",
      "val loss: 0.32531777024269104\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84450: 0.2967110872268677\n",
      "val loss: 0.33069249987602234\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84600: 0.2755875587463379\n",
      "val loss: 0.327902227640152\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84750: 0.29426124691963196\n",
      "val loss: 0.3377900719642639\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84900: 0.2847824692726135\n",
      "val loss: 0.3272211253643036\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85050: 0.29276981949806213\n",
      "val loss: 0.3337973952293396\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85200: 0.2841247618198395\n",
      "val loss: 0.3487209379673004\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85350: 0.29827630519866943\n",
      "val loss: 0.32898837327957153\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85500: 0.26571401953697205\n",
      "val loss: 0.3311452865600586\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85650: 0.2755112648010254\n",
      "val loss: 0.3317907452583313\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85800: 0.2822580337524414\n",
      "val loss: 0.3277284801006317\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85950: 0.34902217984199524\n",
      "val loss: 0.337201863527298\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86100: 0.3251129388809204\n",
      "val loss: 0.3370673358440399\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86250: 0.3249269127845764\n",
      "val loss: 0.3371216058731079\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86400: 0.3371776044368744\n",
      "val loss: 0.3489878177642822\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86550: 0.2933189570903778\n",
      "val loss: 0.3261421024799347\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86700: 0.327156662940979\n",
      "val loss: 0.3332853317260742\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86850: 0.3204495906829834\n",
      "val loss: 0.3324829041957855\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87000: 0.3152562081813812\n",
      "val loss: 0.32369354367256165\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87150: 0.31890785694122314\n",
      "val loss: 0.3592981696128845\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87300: 0.28864359855651855\n",
      "val loss: 0.337384968996048\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87450: 0.2972530424594879\n",
      "val loss: 0.3236577808856964\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87600: 0.2824690043926239\n",
      "val loss: 0.3224654793739319\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87750: 0.3050156533718109\n",
      "val loss: 0.33647817373275757\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87900: 0.299913227558136\n",
      "val loss: 0.33178600668907166\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88050: 0.3097801208496094\n",
      "val loss: 0.33443495631217957\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88200: 0.30328845977783203\n",
      "val loss: 0.32875099778175354\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88350: 0.28153476119041443\n",
      "val loss: 0.3263317048549652\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88500: 0.31576791405677795\n",
      "val loss: 0.32846203446388245\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88650: 0.2969861626625061\n",
      "val loss: 0.3234630525112152\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88800: 0.30530908703804016\n",
      "val loss: 0.3573612570762634\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88950: 0.2831684350967407\n",
      "val loss: 0.34533894062042236\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89100: 0.29724451899528503\n",
      "val loss: 0.32114386558532715\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89250: 0.31682729721069336\n",
      "val loss: 0.3485013544559479\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89400: 0.3131351172924042\n",
      "val loss: 0.32137033343315125\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89550: 0.2842625081539154\n",
      "val loss: 0.3235551416873932\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89700: 0.2777694761753082\n",
      "val loss: 0.3228955864906311\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89850: 0.2973153591156006\n",
      "val loss: 0.3613258898258209\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90000: 0.2629084587097168\n",
      "val loss: 0.32449403405189514\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90150: 0.28666961193084717\n",
      "val loss: 0.33765530586242676\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90300: 0.26317721605300903\n",
      "val loss: 0.3321797847747803\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90450: 0.3121858239173889\n",
      "val loss: 0.3347152769565582\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90600: 0.29462701082229614\n",
      "val loss: 0.33148136734962463\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90750: 0.30003106594085693\n",
      "val loss: 0.359841912984848\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90900: 0.31541571021080017\n",
      "val loss: 0.32195982336997986\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91050: 0.29856887459754944\n",
      "val loss: 0.3501046895980835\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91200: 0.2700593173503876\n",
      "val loss: 0.341778039932251\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91350: 0.30086377263069153\n",
      "val loss: 0.3335028290748596\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91500: 0.3155721127986908\n",
      "val loss: 0.33015570044517517\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91650: 0.31775951385498047\n",
      "val loss: 0.3379165828227997\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91800: 0.2928537428379059\n",
      "val loss: 0.3329342305660248\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91950: 0.31707319617271423\n",
      "val loss: 0.33104774355888367\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92100: 0.29675260186195374\n",
      "val loss: 0.3315373659133911\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92250: 0.30782485008239746\n",
      "val loss: 0.3521205484867096\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92400: 0.2841780185699463\n",
      "val loss: 0.32495322823524475\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92550: 0.2896084487438202\n",
      "val loss: 0.3381527364253998\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92700: 0.30552101135253906\n",
      "val loss: 0.3451417088508606\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92850: 0.2977132797241211\n",
      "val loss: 0.3264367878437042\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93000: 0.2936340868473053\n",
      "val loss: 0.35401904582977295\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93150: 0.31680986285209656\n",
      "val loss: 0.33417758345603943\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93300: 0.30167680978775024\n",
      "val loss: 0.32704874873161316\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93450: 0.3279988765716553\n",
      "val loss: 0.32596153020858765\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93600: 0.3337401747703552\n",
      "val loss: 0.33532679080963135\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93750: 0.2944636046886444\n",
      "val loss: 0.34013301134109497\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93900: 0.28177905082702637\n",
      "val loss: 0.3228885233402252\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94050: 0.32741793990135193\n",
      "val loss: 0.3261006772518158\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94200: 0.3009780943393707\n",
      "val loss: 0.3228331506252289\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94350: 0.26026102900505066\n",
      "val loss: 0.32782241702079773\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94500: 0.30216604471206665\n",
      "val loss: 0.3347499370574951\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94650: 0.3003999888896942\n",
      "val loss: 0.3172437250614166\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94800: 0.30267205834388733\n",
      "val loss: 0.339354932308197\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94950: 0.29269394278526306\n",
      "val loss: 0.3258730471134186\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95100: 0.30712056159973145\n",
      "val loss: 0.3217412531375885\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95250: 0.40742215514183044\n",
      "val loss: 0.4521884322166443\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95400: 0.33176979422569275\n",
      "val loss: 0.33119645714759827\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95550: 0.30975016951560974\n",
      "val loss: 0.33213719725608826\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95700: 0.3321619927883148\n",
      "val loss: 0.346397340297699\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95850: 0.30621421337127686\n",
      "val loss: 0.32507482171058655\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96000: 0.28100821375846863\n",
      "val loss: 0.3255288600921631\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96150: 0.2795911431312561\n",
      "val loss: 0.34359681606292725\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96300: 0.2859964668750763\n",
      "val loss: 0.32416781783103943\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96450: 0.28735285997390747\n",
      "val loss: 0.33466488122940063\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96600: 0.29438263177871704\n",
      "val loss: 0.3611633777618408\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96750: 0.2871086299419403\n",
      "val loss: 0.3651531934738159\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96900: 0.29550257325172424\n",
      "val loss: 0.3510536849498749\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97050: 0.2922005355358124\n",
      "val loss: 0.33792775869369507\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97200: 0.28186580538749695\n",
      "val loss: 0.3455640971660614\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97350: 0.2816517949104309\n",
      "val loss: 0.3269544541835785\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97500: 0.2714183032512665\n",
      "val loss: 0.3313674330711365\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97650: 0.2892095148563385\n",
      "val loss: 0.3287115693092346\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97800: 0.24887537956237793\n",
      "val loss: 0.326400488615036\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97950: 0.2524107098579407\n",
      "val loss: 0.32862088084220886\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98100: 0.32524749636650085\n",
      "val loss: 0.36879169940948486\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98250: 0.32036805152893066\n",
      "val loss: 0.3448421061038971\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98400: 0.3066628873348236\n",
      "val loss: 0.312406450510025\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98550: 0.2806648313999176\n",
      "val loss: 0.31701549887657166\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98700: 0.28900954127311707\n",
      "val loss: 0.3282930850982666\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98850: 0.3805670440196991\n",
      "val loss: 0.38445159792900085\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99000: 0.34316226840019226\n",
      "val loss: 0.3787195682525635\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99150: 0.3203515410423279\n",
      "val loss: 0.35842978954315186\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99300: 0.3439550995826721\n",
      "val loss: 0.3642039895057678\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99450: 0.34072402119636536\n",
      "val loss: 0.35695067048072815\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99600: 0.3143106997013092\n",
      "val loss: 0.3140757083892822\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99750: 0.2918449342250824\n",
      "val loss: 0.332396000623703\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99900: 0.29401928186416626\n",
      "val loss: 0.32520076632499695\n",
      "\n",
      "\n",
      "*****************************************************************************************************Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 100050: 0.2933352589607239\n",
      "val loss: 0.33680030703544617\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 100200: 0.2960361838340759\n",
      "val loss: 0.32916060090065\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 100350: 0.3781423270702362\n",
      "val loss: 0.4060061573982239\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 100500: 0.3958669602870941\n",
      "val loss: 0.45682910084724426\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 100650: 0.33730030059814453\n",
      "val loss: 0.3625045418739319\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 100800: 0.30292585492134094\n",
      "val loss: 0.3213249742984772\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 100950: 0.3273276686668396\n",
      "val loss: 0.3205898702144623\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 101100: 0.3233398199081421\n",
      "val loss: 0.3551315367221832\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 101250: 0.30947571992874146\n",
      "val loss: 0.38694071769714355\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 101400: 0.3168165683746338\n",
      "val loss: 0.359536737203598\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 101550: 0.3505402207374573\n",
      "val loss: 0.4213666319847107\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 101700: 0.3250996172428131\n",
      "val loss: 0.34318968653678894\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 101850: 0.30121833086013794\n",
      "val loss: 0.35348445177078247\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 102000: 0.28162774443626404\n",
      "val loss: 0.32872772216796875\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 102150: 0.31256166100502014\n",
      "val loss: 0.31485220789909363\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 102300: 0.2873254418373108\n",
      "val loss: 0.33434414863586426\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 102450: 0.35624608397483826\n",
      "val loss: 0.3990948796272278\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 102600: 0.3142413794994354\n",
      "val loss: 0.33843258023262024\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*\n",
      "\n",
      "overall_steps 102750: 0.3094542920589447\n",
      "val loss: 0.3286871314048767\n",
      "\n",
      "\n",
      "Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n",
      "*Complete training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/scratch2/nkudryas/BAND-torch/BCI_ALVI_challenge/utils/train.py:102\u001b[0m, in \u001b[0;36mrun_train_model\u001b[0;34m(model, datasets, config, device)\u001b[0m\n\u001b[1;32m     99\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    103\u001b[0m         lr \u001b[38;5;241m=\u001b[39m scheduler(overall_step)\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m/disk/scratch2/nkudryas/micromamba/envs/bci-kaggle/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/disk/scratch2/nkudryas/micromamba/envs/bci-kaggle/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin_memory_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/disk/scratch2/nkudryas/micromamba/envs/bci-kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:88\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m---> 88\u001b[0m         clone[i] \u001b[38;5;241m=\u001b[39m \u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/disk/scratch2/nkudryas/micromamba/envs/bci-kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py:58\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "run_train_model(model, (train_dataset, test_dataset), tterain_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x7f713af64190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
